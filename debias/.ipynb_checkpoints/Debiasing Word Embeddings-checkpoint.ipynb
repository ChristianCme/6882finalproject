{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Double-Hard Debias: [https://github.com/uvavision/Double-Hard-Debias/blob/master/GloVe_Debias.ipynb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import scipy\n",
    "import codecs, os, json\n",
    "import operator\n",
    "import pickle\n",
    "from random import shuffle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading Embeddings: Double Hard Debias\n",
    "\n",
    "def normalize(wv):    \n",
    "    # normalize vectors\n",
    "    norms = np.apply_along_axis(LA.norm, 1, wv)\n",
    "    wv = wv / norms[:, np.newaxis]\n",
    "    return wv\n",
    "\n",
    "def load_w2v(file_path):\n",
    "    # load w2v file format\n",
    "    model =gensim.models.KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "    vocab = sorted([w for w in model.vocab], key=lambda w: model.vocab[w].index)\n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    wv = [model[w] for w in vocab]\n",
    "    wv = np.array(wv)\n",
    "    print(len(vocab), wv.shape, len(w2i))\n",
    "    \n",
    "    return wv, w2i, vocab\n",
    "\n",
    "def load_embeddings_p(path):\n",
    "    # load pickle files (double hard debias)\n",
    "    debiased_embeds = pickle.load(open(path, 'rb'))\n",
    "    wv = []\n",
    "    vocab = []\n",
    "    for w in debiased_embeds:\n",
    "        wv.append(np.array(debiased_embeds[w]))\n",
    "        vocab.append(str(w))\n",
    "        \n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    wv = np.array(wv).astype(float)\n",
    "    print(len(vocab), wv.shape, len(w2i))\n",
    "        \n",
    "    return wv, w2i, vocab\n",
    "\n",
    "def load_embeddings_from_np(filename):\n",
    "    #print('loading ...')\n",
    "    with codecs.open(filename + '.vocab', 'r', 'utf-8') as f_embed:\n",
    "        vocab = [line.strip() for line in f_embed]\n",
    "        \n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    wv = np.load(filename + '.wv.npy')\n",
    "    \n",
    "    print(len(vocab), wv.shape, len(w2i))\n",
    "\n",
    "    return vocab, wv, w2i\n",
    "\n",
    "def save_embeds(wv, vocab, filename):\n",
    "    out_emb_file = filename + '.wv'\n",
    "    out_vocab_file = filename + '.vocab'\n",
    "    \n",
    "    #print('Saving binary file to {}'.format(out_emb_file))\n",
    "    np.save(out_emb_file, wv)\n",
    "\n",
    "    #print('Saving vocabulary file to {}'.format(out_vocab_file))\n",
    "    with codecs.open(out_vocab_file, 'w', 'utf-8') as f_out:\n",
    "        for word in vocab:\n",
    "            f_out.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322636 (322636, 300) 322636\n"
     ]
    }
   ],
   "source": [
    "# load Double Hard Debiased Embeddings (GloVe)\n",
    "# GloVe\n",
    "dhd_glove, dhd_glove_w2i, dhd_vocab = load_embeddings_p(\"../data/Wang/glove_dhd.p\")\n",
    "# save pickle embeds as npy arrays\n",
    "save_embeds(dhd_glove, dhd_vocab, '../data/Wang/dhd_glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000 (3000000, 300) 3000000\n"
     ]
    }
   ],
   "source": [
    "# load Double Hard Debiased Embeddings (W2V)\n",
    "# Word2Vec\n",
    "dhd_wv, dhd_w2i, dhd_vocab = load_embeddings_p(\"../data/Wang/dhd_wv.p\")\n",
    "# save pickle embeds as npy arrays\n",
    "save_embeds(dhd_wv, dhd_vocab, '../data/Wang/dhd_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322636 (322636, 300) 322636\n"
     ]
    }
   ],
   "source": [
    "# load gender neutral GloVe Embeddings\n",
    "gn_glove, gn_glove_w2i, gn_vocab = load_embeddings_from_np(\"../data/Zhao/gn_glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322636 (322636, 300) 322636\n"
     ]
    }
   ],
   "source": [
    "# load original GloVe Embeddings\n",
    "glove, glove_w2i, glove_vocab = load_embeddings_from_np(\"../data/Zhao/orig_glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999996 (2999996, 300) 2999996\n"
     ]
    }
   ],
   "source": [
    "# load original w2v Embeddings\n",
    "w2v, w2v_w2i, w2v_vocab = load_embeddings_from_np(\"../data/Bolubaski/orig_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999996 (2999996, 300) 2999996\n"
     ]
    }
   ],
   "source": [
    "# load Hard Debiased w2v Embeddings\n",
    "hd_w2v, hd_w2v_w2i, hd_vocab = load_embeddings_from_np(\"../data/Bolubaski/hard_debiased_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# get main PCA components\n",
    "def my_pca(wv):\n",
    "    wv_mean = np.mean(np.array(wv), axis=0)\n",
    "    wv_hat = np.zeros(wv.shape).astype(float)\n",
    "\n",
    "    for i in range(len(wv)):\n",
    "        wv_hat[i, :] = wv[i, :] - wv_mean\n",
    "\n",
    "    main_pca = PCA()\n",
    "    main_pca.fit(wv_hat)\n",
    "    \n",
    "    return main_pca\n",
    "\n",
    "main_pca = my_pca(wv)\n",
    "wv_mean = np.mean(np.array(wv), axis=0)\n",
    "\n",
    "# Bolukbasi projection-based debiasing method\n",
    "def hard_debias(wv, w2i, w2i_partial, vocab_partial, component_ids):\n",
    "    \n",
    "    D = []\n",
    "\n",
    "    for i in component_ids:\n",
    "        D.append(main_pca.components_[i])\n",
    "    \n",
    "    # get rid of frequency features\n",
    "    wv_f = np.zeros((len(vocab_partial), wv.shape[1])).astype(float)\n",
    "    \n",
    "    for i, w in enumerate(vocab_partial):\n",
    "        u = wv[w2i[w], :]\n",
    "        sub = np.zeros(u.shape).astype(float)\n",
    "        for d in D:\n",
    "            sub += np.dot(np.dot(np.transpose(d), u), d)\n",
    "        wv_f[w2i_partial[w], :] = wv[w2i[w], :] - sub - wv_mean\n",
    "        \n",
    "    # debias\n",
    "    gender_directions = list()\n",
    "    for gender_word_list in [definitional_pairs]:\n",
    "        gender_directions.append(doPCA(gender_word_list, wv_f, w2i_partial).components_[0])\n",
    "    \n",
    "    wv_debiased = np.zeros((len(vocab_partial), len(wv_f[0, :]))).astype(float)\n",
    "    for i, w in enumerate(vocab_partial):\n",
    "        u = wv_f[w2i_partial[w], :]\n",
    "        for gender_direction in gender_directions:\n",
    "            u = drop(u, gender_direction)\n",
    "            wv_debiased[w2i_partial[w], :] = u\n",
    "    \n",
    "    return wv_debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
