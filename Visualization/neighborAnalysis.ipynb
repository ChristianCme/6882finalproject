{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63254230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot number of male neighbors vs original bias\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd64baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9dc0fa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from util import preprocessWordVecs, removeWords, load_legacy_w2v\n",
    "from loader import load_def_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d173bd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('biased_embeddings', help=\"Path to biased embeddings\")\n",
    "    parser.add_argument('debiased_embeddings', help=\"Path to debiased embeddings\")\n",
    "    parser.add_argument('vocab_path', help=\"Path to JSON file containing debiasing info\")\n",
    "    parser.add_argument('targets',\n",
    "                        help='JSON file containing list of target words to evaluate bias on')\n",
    "\n",
    "    parser.add_argument('--multi', action='store_true',\n",
    "                        help=\"\"\"Set this flag for multiclass debiasing. Binary allows for\n",
    "                        more compact plots.\"\"\")\n",
    "\n",
    "    parser.add_argument('--bias_specific',\n",
    "                        help=\"\"\"JSON file containing list of words\n",
    "                        that inherently contain bias and should be ignored\"\"\")\n",
    "\n",
    "    parser.add_argument('-v', '--verbose', action='store_true')\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54b637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_biased(word_vectors, subspace, n_biased=500):\n",
    "    \"\"\"\n",
    "    Get vectors with most positive and negative bias wrt subspace.\n",
    "    \"\"\"\n",
    "\n",
    "    biases = {}\n",
    "\n",
    "    for word, vector in word_vectors.items():\n",
    "        # 1d case\n",
    "        bias = np.dot(vector, subspace)\n",
    "        biases[word] = bias\n",
    "\n",
    "    sorted_biases = sorted(list(biases.items()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    positive_bias = sorted_biases[:n_biased]\n",
    "    negative_bias = list(reversed(sorted_biases[-n_biased:]))\n",
    "\n",
    "    return positive_bias, negative_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066efdb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_multi(args):\n",
    "    \"\"\"\n",
    "    For multi-class: bias direction is from mean to extreme\n",
    "    \"\"\"\n",
    "\n",
    "    # load embeddings\n",
    "    print(\"Loading word embeddings...\")\n",
    "    word_vectors, embed_dim = load_legacy_w2v(args.biased_embeddings)\n",
    "    debiased_vectors, _ = load_legacy_w2v(args.debiased_embeddings)\n",
    "\n",
    "    # prune\n",
    "    word_vectors = preprocessWordVecs(word_vectors)\n",
    "    debiased_vectors = preprocessWordVecs(debiased_vectors)\n",
    "\n",
    "    def_sets = load_def_sets(args.vocab_path)\n",
    "\n",
    "    # take first row of def_sets as defining\n",
    "    classes = def_sets[0]\n",
    "\n",
    "    vects = np.zeros((len(classes), embed_dim))\n",
    "    for i, word in enumerate(classes):\n",
    "        if word not in word_vectors:\n",
    "            raise ValueError(word)\n",
    "        vects[i] = word_vectors[word]\n",
    "    mean = vects.mean(0)\n",
    "\n",
    "    directions = np.zeros((len(classes), embed_dim))\n",
    "    for i, word in enumerate(classes):\n",
    "        directions[i] = word_vectors[word] - mean\n",
    "        directions[i] = directions[i] / np.linalg.norm(directions[i])\n",
    "\n",
    "    # remove bias-specific words\n",
    "    if args.bias_specific is not None:\n",
    "        biased = json.load(open(args.bias_specific, 'r'))\n",
    "    else:\n",
    "        biased = []\n",
    "    for value in def_sets.values():\n",
    "        biased.extend(value)\n",
    "\n",
    "    word_vectors = removeWords(word_vectors, biased)\n",
    "    debiased_vectors = removeWords(debiased_vectors, biased)\n",
    "\n",
    "    targets = json.load(open(args.targets, 'r'))\n",
    "    targets = [x[0] for x in targets]\n",
    "    # compute bias for all targets\n",
    "    original_bias = {}\n",
    "\n",
    "    for i, c in enumerate(classes):\n",
    "        original_bias[c] = {}\n",
    "        for word in targets:\n",
    "            if word in word_vectors:\n",
    "                original_bias[c][word] = np.dot(word_vectors[word], directions[i])\n",
    "\n",
    "    print(len(original_bias[classes[0]]))\n",
    "    # get most biased targets\n",
    "    for c, orig_bias in original_bias.items():\n",
    "        sorted_prof_bias = sorted(orig_bias.items(), key=lambda x: x[1], reverse=True)\n",
    "        most_biased_profs = sorted_prof_bias[:10]\n",
    "        most_antibiased_profs = list(reversed(sorted_prof_bias[-10:]))\n",
    "        if args.verbose:\n",
    "            print(\"Ten most {}-biased targets\".format(c))\n",
    "            print(most_biased_profs)\n",
    "            print(\"Ten most anti-{}-biased targets\".format(c))\n",
    "            print(most_antibiased_profs)\n",
    "            print()\n",
    "\n",
    "    # get 500 most biased words in either direction\n",
    "    positive_words = {}\n",
    "    negative_words = {}\n",
    "    print(\"computing bias of all words in each direction\")\n",
    "    for i, c in enumerate(classes):\n",
    "        positive_bias, negative_bias = get_most_biased(word_vectors, directions[i])\n",
    "        print(\"obtained 500 most positive- and negative-biased words for {}\".format(c))\n",
    "\n",
    "        if args.verbose:\n",
    "            print(\"Ten most biased for {}:\".format(c))\n",
    "            print(positive_bias[:10])\n",
    "            print(\"Ten most anti-biased for {}\".format(c))\n",
    "            print(negative_bias[:10])\n",
    "            print()\n",
    "\n",
    "        positive_words[c] = [x[0] for x in positive_bias]\n",
    "        negative_words[c] = [x[0] for x in negative_bias]\n",
    "\n",
    "    n_closest = {}\n",
    "    for i, c in enumerate(classes):\n",
    "        pos_words = positive_words[c]\n",
    "        neg_words = negative_words[c]\n",
    "        # get vectors in biased and debiased embeddings\n",
    "        print(\"extracting subset of most biased words for {}\".format(c))\n",
    "        subset_size = len(pos_words) + len(neg_words)\n",
    "        biased_subset = {}\n",
    "        debiased_subset = {}\n",
    "        for i, word in enumerate(pos_words + neg_words):\n",
    "            biased_subset[word] = word_vectors[word]\n",
    "            debiased_subset[word] = debiased_vectors[word]\n",
    "\n",
    "        n_closest[c] = {}\n",
    "        # for each profession, find number of closest numbers that are in the positive class\n",
    "        for word in original_bias[c].keys():\n",
    "            # compute distance to all words in subset\n",
    "            biased_distances = {}\n",
    "            vec = word_vectors[word]\n",
    "            for target, t_vec in biased_subset.items():\n",
    "                biased_distances[target] = np.linalg.norm(t_vec - vec)\n",
    "\n",
    "            debiased_distances = {}\n",
    "            vec = debiased_vectors[word]\n",
    "            for target, t_vec in debiased_subset.items():\n",
    "                debiased_distances[target] = np.linalg.norm(t_vec - vec)\n",
    "\n",
    "            # get 100 closest neighbors, count positive class\n",
    "            closest_biased = sorted(biased_distances.items(), key=lambda x: x[1])[:100]\n",
    "            closest_debiased = sorted(debiased_distances.items(), key=lambda x: x[1])[:100]\n",
    "\n",
    "            n_positive = [0, 0]\n",
    "            for word2, _ in closest_biased:\n",
    "                if word2 in pos_words:\n",
    "                    n_positive[0] += 1\n",
    "\n",
    "            for word2, _ in closest_debiased:\n",
    "                if word2 in pos_words:\n",
    "                    n_positive[1] += 1\n",
    "\n",
    "            n_closest[c][word] = n_positive\n",
    "\n",
    "    print(len(original_bias[classes[0]]))\n",
    "    print(len(n_closest[classes[0]]))\n",
    "\n",
    "    for c in classes:\n",
    "        try:\n",
    "            assert set(n_closest[c].keys()) == set(original_bias[c].keys())\n",
    "        except Exception as e:\n",
    "            print(set(n_closest[c].keys()).symmetric_difference(set(original_bias[c].keys())))\n",
    "            raise e\n",
    "\n",
    "    # plot number of positive neighbors vs. original bias\n",
    "    figs = []\n",
    "    for c in classes:\n",
    "        biases = []\n",
    "        n_neighbors_biased = []\n",
    "        n_neighbors_debiased = []\n",
    "        for word, bias in original_bias[c].items():\n",
    "            biases.append(bias)\n",
    "            n_neighbors_biased.append(n_closest[c][word][0])\n",
    "            n_neighbors_debiased.append(n_closest[c][word][1])\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 2))\n",
    "        ax = fig.add_subplot(111)\n",
    "        # plt.subplot(121)\n",
    "        plt.scatter(biases, n_neighbors_biased, s=1.5, color='c')\n",
    "        # plt.title(\"Original\", fontsize='medium')\n",
    "        ax.text(0.03, 0.88, \"Original\", fontsize='small', transform=ax.transAxes,\n",
    "            horizontalalignment='left')\n",
    "        plt.ylim(0, 100)\n",
    "        plt.tick_params(labelsize=8)\n",
    "\n",
    "        # plot most biased targets\n",
    "        for p, _ in most_biased_profs[:7] + most_antibiased_profs[:7]:\n",
    "            x = original_bias[c][p]\n",
    "            y = n_closest[c][p][0]\n",
    "            # print(p, (x, y))\n",
    "            plt.annotate(p, xy=(x, y),\n",
    "                xytext=(np.random.random()*0.1, np.random.random()*0.1),\n",
    "                textcoords='offset pixels', fontsize='x-small')\n",
    "\n",
    "        plt.show()\n",
    "        figs.append(fig)\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 2))\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.scatter(biases, n_neighbors_debiased, s=1.5, color='c')\n",
    "        ax.text(0.03, 0.88, f\"Debiased\", fontsize='small', transform=ax.transAxes,\n",
    "            horizontalalignment='left')\n",
    "        plt.ylim(0, 100)\n",
    "        plt.tick_params(labelsize=8)\n",
    "\n",
    "        for p, _ in most_biased_profs[:7] + most_antibiased_profs[:7]:\n",
    "            # print(p)\n",
    "            x = original_bias[c][p]\n",
    "            y = n_closest[c][p][1]\n",
    "            plt.annotate(p, xy=(x, y),\n",
    "                xytext=(np.random.random()*0.1, np.random.random()*0.1),\n",
    "                textcoords='offset pixels', fontsize='x-small')\n",
    "\n",
    "        plt.show()\n",
    "        figs.append(fig)\n",
    "        \n",
    "        print(c)\n",
    "        print(\"biased\")\n",
    "        print(\"Pearson: {}\".format(pearsonr(biases, n_neighbors_biased)))\n",
    "        print(\"Spearman: {}\".format(spearmanr(biases, n_neighbors_biased)))\n",
    "        print(\"debiased\")\n",
    "        print(\"Pearson: {}\".format(pearsonr(biases, n_neighbors_debiased)))\n",
    "        print(\"Spearman: {}\".format(spearmanr(biases, n_neighbors_debiased)))\n",
    "\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b055e0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_binary(args):\n",
    "    # load embeddings\n",
    "    print(\"Loading word embeddings...\")\n",
    "    word_vectors, embed_dim = load_legacy_w2v(args.biased_embeddings)\n",
    "    debiased_vectors, _ = load_legacy_w2v(args.debiased_embeddings)\n",
    "\n",
    "    # prune\n",
    "    word_vectors = preprocessWordVecs(word_vectors)\n",
    "    debiased_vectors = preprocessWordVecs(debiased_vectors)\n",
    "\n",
    "    # assume gender direction is just he - she\n",
    "    gender_direction = word_vectors['he'] - word_vectors['she']\n",
    "    gender_direction = gender_direction / np.linalg.norm(gender_direction)\n",
    "    debiased_direction = debiased_vectors['he'] - debiased_vectors['she']\n",
    "\n",
    "    # remove gender-specific words\n",
    "    def_sets = load_def_sets(args.vocab_path)\n",
    "    if args.bias_specific is not None:\n",
    "        biased = json.load(open(args.bias_specific, 'r'))\n",
    "    else:\n",
    "        biased = []\n",
    "\n",
    "    for value in def_sets.values():\n",
    "        biased.extend(value)\n",
    "    word_vectors = removeWords(word_vectors, biased)\n",
    "    debiased_vectors = removeWords(debiased_vectors, biased)\n",
    "\n",
    "    targets = json.load(open(args.targets, 'r'))\n",
    "    targets = [x[0] for x in targets]\n",
    "    # compute bias for all targets\n",
    "    original_bias = {}\n",
    "    for word in targets:\n",
    "        if word in word_vectors:\n",
    "            original_bias[word] = np.dot(word_vectors[word], gender_direction)\n",
    "\n",
    "    print(len(original_bias))\n",
    "    # get most biased targets\n",
    "    sorted_prof_bias = sorted(original_bias.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if args.verbose:\n",
    "        print(\"Ten most male-biased targets\")\n",
    "        print(sorted_prof_bias[:10])\n",
    "        print(\"Ten most female-biased targets\")\n",
    "        print(list(reversed(sorted_prof_bias[-10:])))\n",
    "        print()\n",
    "\n",
    "    # get 500 most biased words in either direction\n",
    "    print(\"computing bias of all words\")\n",
    "    positive_bias, negative_bias = get_most_biased(word_vectors, gender_direction)\n",
    "    \n",
    "    if args.verbose:\n",
    "        print(\"Ten most male-biased\")\n",
    "        print(positive_bias[:10])\n",
    "        print(\"Ten most anti-male-biased\")\n",
    "        print(negative_bias[:10])\n",
    "        print()\n",
    "\n",
    "    print(\"obtained 500 most positive- and negative-biased words\")\n",
    "    positive_words = [x[0] for x in positive_bias]\n",
    "    negative_words = [x[0] for x in negative_bias]\n",
    "\n",
    "    # get vectors in biased and debiased embeddings\n",
    "    print(\"extracting subset of most biased words\")\n",
    "    subset_size = len(positive_words) + len(negative_words)\n",
    "    # biased_subset = np.zeros((subset_size, embed_dim))\n",
    "    # debiased_subset = np.zeros_like(biased_subset)\n",
    "    biased_subset = {}\n",
    "    debiased_subset = {}\n",
    "    for i, word in enumerate(positive_words + negative_words):\n",
    "        biased_subset[word] = word_vectors[word]\n",
    "        debiased_subset[word] = debiased_vectors[word]\n",
    "\n",
    "    # for each profession, find number of closest numbers that are in the positive class\n",
    "    n_closest = {}\n",
    "    for word in original_bias.keys():\n",
    "        # compute distance to all words in subset\n",
    "        biased_distances = {}\n",
    "        vec = word_vectors[word]\n",
    "        for target, t_vec in biased_subset.items():\n",
    "            biased_distances[target] = np.linalg.norm(t_vec - vec)\n",
    "\n",
    "        debiased_distances = {}\n",
    "        vec = debiased_vectors[word]\n",
    "        for target, t_vec in debiased_subset.items():\n",
    "            debiased_distances[target] = np.linalg.norm(t_vec - vec)\n",
    "\n",
    "        # get 100 closest neighbors, count positive class\n",
    "        closest_biased = sorted(biased_distances.items(), key=lambda x: x[1])[:100]\n",
    "        closest_debiased = sorted(debiased_distances.items(), key=lambda x: x[1])[:100]\n",
    "\n",
    "        n_positive = [0, 0]\n",
    "        for word2, _ in closest_biased:\n",
    "            if word2 in positive_words:\n",
    "                n_positive[0] += 1\n",
    "\n",
    "        for word2, _ in closest_debiased:\n",
    "            if word2 in positive_words:\n",
    "                n_positive[1] += 1\n",
    "\n",
    "        n_closest[word] = n_positive\n",
    "\n",
    "    print(len(original_bias))\n",
    "    print(len(n_closest))\n",
    "\n",
    "    try:\n",
    "        assert set(n_closest.keys()) == set(original_bias.keys())\n",
    "    except Exception as e:\n",
    "        print(set(n_closest.keys()).symmetric_difference(set(original_bias.keys())))\n",
    "        raise e\n",
    "\n",
    "    # plot number of positive neighbors vs. original bias\n",
    "    biases = []\n",
    "    n_neighbors_biased = []\n",
    "    n_neighbors_debiased = []\n",
    "    for word, bias in original_bias.items():\n",
    "        biases.append(bias)\n",
    "        n_neighbors_biased.append(n_closest[word][0])\n",
    "        n_neighbors_debiased.append(n_closest[word][1])\n",
    "\n",
    "    plt.figure(figsize=(10, 3.5))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(biases, n_neighbors_biased, s=1.5)\n",
    "    plt.title(\"Original\")\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(biases, n_neighbors_debiased, s=2)\n",
    "    plt.title(\"Debiased\")\n",
    "\n",
    "    print(\"Pearson: {}\".format(pearsonr(biases, n_neighbors_biased)))\n",
    "    print(\"Spearman: {}\".format(spearmanr(biases, n_neighbors_biased)))\n",
    "    print(\"Pearson (debiased): {}\".format(pearsonr(biases, n_neighbors_debiased)))\n",
    "    print(\"Spearman (debiased): {}\".format(spearmanr(biases, n_neighbors_debiased)))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2bb0cc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    if args.multi:\n",
    "        plot_multi(args)\n",
    "    else:\n",
    "        plot_binary(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
